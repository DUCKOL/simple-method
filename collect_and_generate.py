import requests
from bs4 import BeautifulSoup
import re
import urllib.parse
import sys

# --- VLESS é“¾æ¥çš„æ ¸å¿ƒé…ç½®å‚æ•° ---
USER_ID = '2e24b381-09e5-4386-b8c2-033c1bc40b12'
PORT = 443
HOST = 'ysun74.ysunyang.qzz.io'
PATH = '/?ed=2560'
REMARKS_PREFIX = '@ys==>unlock all'
OUTPUT_FILE = 'vless_links.txt' # æœ€ç»ˆè¾“å‡ºçš„æ–‡ä»¶å

# --- ç”¨äºæŠ“å– IP çš„ç›®æ ‡ç½‘å€åˆ—è¡¨ ---
URLS_TO_SCRAPE = [
    'https://api.uouin.com/cloudflare.html', 
    'https://ip.164746.xyz'
]

# ç”¨äºä»æ–‡æœ¬ä¸­åŒ¹é… IP åœ°å€çš„æ­£åˆ™è¡¨è¾¾å¼
IP_PATTERN = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'

def scrape_live_ips():
    """
    ä»æ‰€æœ‰ç›®æ ‡ URL æŠ“å– IP åœ°å€ã€‚
    è¿”å›ä¸€ä¸ªå»é‡åçš„ IP åœ°å€é›†åˆã€‚
    """
    unique_ips = set()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print("ğŸš€ å¼€å§‹ä»ä»¥ä¸‹ URL æŠ“å–å®æ—¶ IP åœ°å€:")
    for url in URLS_TO_SCRAPE:
        print(f"  - æ­£åœ¨è®¿é—®: {url}")
        try:
            # å‘èµ·ç½‘ç»œè¯·æ±‚ï¼Œè®¾ç½®10ç§’è¶…æ—¶
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status() # å¦‚æœè¯·æ±‚çŠ¶æ€ç ä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML å†…å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ ¼å¼çš„ IP åœ°å€
            ip_matches = re.findall(IP_PATTERN, soup.get_text())
            
            if ip_matches:
                unique_ips.update(ip_matches)
                print(f"    âœ… åœ¨æ­¤ URL æ‰¾åˆ° {len(ip_matches)} ä¸ª IP åœ°å€ã€‚")
            else:
                print(f"    âš ï¸ åœ¨æ­¤ URL æœªæ‰¾åˆ°ä»»ä½• IP åœ°å€ã€‚")

        except requests.RequestException as e:
            print(f"    âŒ è®¿é—® URL å¤±è´¥: {e}")
            
    return unique_ips

def generate_vless_links(ip_set):
    """
    æ ¹æ®æä¾›çš„ IP åœ°å€é›†åˆï¼Œç”Ÿæˆ VLESS é“¾æ¥åˆ—è¡¨ã€‚
    """
    all_vless_links = []
    print("\nğŸ”§ å¼€å§‹ä¸ºæ¯ä¸ªå”¯ä¸€çš„ IP åœ°å€ç”Ÿæˆ VLESS é“¾æ¥...")

    # å¯¹ IP è¿›è¡Œæ’åºï¼Œä»¥ç¡®ä¿æ¯æ¬¡ç”Ÿæˆçš„æ–‡ä»¶å†…å®¹é¡ºåºä¸€è‡´
    for ip in sorted(list(ip_set)): 
        # address éƒ¨åˆ†ç›´æ¥ä½¿ç”¨æŠ“å–åˆ°çš„ IP
        address = ip
        
        # åˆ›å»ºç¬¦åˆæ ¼å¼çš„åˆ«å: @ys==>unlock all [IPåœ°å€]
        remarks = f"{REMARKS_PREFIX} {ip}"
        
        # ä¸ºäº† URL å®‰å…¨ï¼Œå¯¹è·¯å¾„å’Œåˆ«åä¸­çš„ç‰¹æ®Šå­—ç¬¦è¿›è¡Œç¼–ç 
        encoded_path = urllib.parse.quote(PATH)
        encoded_remarks = urllib.parse.quote(remarks)
        
        # æŒ‰ç…§ VLESS æ ¼å¼æ‹¼æ¥å®Œæ•´çš„é“¾æ¥å­—ç¬¦ä¸²
        vless_link = (
            f"vless://{USER_ID}@{address}:{PORT}?"
            f"encryption=none&security=tls&type=ws&host={HOST}&path={encoded_path}"
            f"#{encoded_remarks}"
        )
        all_vless_links.append(vless_link)
        
    return all_vless_links

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # 1. æ‰§è¡Œ IP æŠ“å–
    scraped_ips = scrape_live_ips()
    
    if not scraped_ips:
        print("\nâŒ æœªèƒ½æŠ“å–åˆ°ä»»ä½•æœ‰æ•ˆçš„ IP åœ°å€ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        sys.exit(1) # é€€å‡ºå¹¶è¿”å›é”™è¯¯ç ï¼ŒGitHub Actions ä¼šå°†æ­¤æ¬¡è¿è¡Œæ ‡è®°ä¸ºå¤±è´¥
        
    print(f"\nâœ¨ æŠ“å–å®Œæˆï¼å…±æ‰¾åˆ° {len(scraped_ips)} ä¸ªä¸é‡å¤çš„ IP åœ°å€ã€‚")
    
    # 2. æ ¹æ®æŠ“å–åˆ°çš„ IP ç”Ÿæˆé“¾æ¥
    vless_links = generate_vless_links(scraped_ips)
    
    # 3. å°†æ‰€æœ‰ç”Ÿæˆçš„é“¾æ¥å†™å…¥æ–‡ä»¶
    # ä½¿ç”¨ 'w' æ¨¡å¼å†™å…¥ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡è¿è¡Œæ—¶éƒ½ä¼šæ¸…ç©ºå¹¶é‡å†™æ–‡ä»¶ï¼Œè¾¾åˆ°è¦†ç›–æ•ˆæœã€‚
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            # ä½¿ç”¨æ¢è¡Œç¬¦ '\n' å°†åˆ—è¡¨ä¸­çš„æ¯ä¸ªé“¾æ¥è¿æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²åå†™å…¥æ–‡ä»¶
            f.write('\n'.join(vless_links))
        print(f"\nâœ… æˆåŠŸï¼æ‰€æœ‰ {len(vless_links)} æ¡é“¾æ¥å·²è¦†ç›–å†™å…¥åˆ° '{OUTPUT_FILE}' æ–‡ä»¶ä¸­ã€‚")
    except IOError as e:
        print(f"\nâŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1) # å†™å…¥å¤±è´¥åˆ™ç»ˆæ­¢ç¨‹åº
