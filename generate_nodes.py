import requests
from bs4 import BeautifulSoup
import re
import urllib.parse
import sys
import random

# --- VLESS é…ç½®å‚æ•° ---
USER_ID = '2e24b381-09e5-4386-b8c2-033c1bc40b12'
PORT = 443
PATH = '/?ed=2560'
# ==========================================================
# VVVVVVVV  è¿™é‡Œæ˜¯æ ¹æ®æ‚¨çš„è¦æ±‚ä¿®æ”¹çš„åœ°æ–¹  VVVVVVVV
REMARKS_PREFIX = '@DUCKOL(Github)'  # <-- å·²å°†åˆ«åå‰ç¼€ä¿®æ”¹ä¸ºæ‚¨æŒ‡å®šçš„åç§°
# ^^^^^^^^  è¿™é‡Œæ˜¯æ ¹æ®æ‚¨çš„è¦æ±‚ä¿®æ”¹çš„åœ°æ–¹  ^^^^^^^^
# ==========================================================
OUTPUT_FILE = 'vless_links.txt'
DOMAINS_FILE = 'domains.txt'

# --- ç›®æ ‡URLåˆ—è¡¨ï¼Œç”¨äºæŠ“å–IP ---
URLS_TO_SCRAPE = [
    'https://api.uouin.com/cloudflare.html', 
    'https://ip.164746.xyz'
]
IP_PATTERN = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'

def scrape_ips():
    """ä»ç›®æ ‡URLæŠ“å–IPåœ°å€å¹¶è¿”å›ä¸€ä¸ªå»é‡åçš„åˆ—è¡¨"""
    unique_ips = set()
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    print("ğŸš€ å¼€å§‹æŠ“å–IPåœ°å€...")
    for url in URLS_TO_SCRAPE:
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            ip_matches = re.findall(IP_PATTERN, response.text)
            if ip_matches:
                unique_ips.update(ip_matches)
        except requests.RequestException as e:
            print(f"    âŒ è®¿é—®URL {url} å¤±è´¥: {e}")
            
    return list(unique_ips)

def read_domains():
    """ä» domains.txt è¯»å–åŸŸååˆ—è¡¨ """
    try:
        with open(DOMAINS_FILE, 'r') as f:
            domains = [line.strip() for line in f if line.strip()]
        return domains
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: åŸŸåæ–‡ä»¶ '{DOMAINS_FILE}' æœªæ‰¾åˆ°ã€‚")
        return []

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    # 1. æŠ“å–IP
    live_ips = scrape_ips()
    if not live_ips:
        print("âŒ æœªèƒ½æŠ“å–åˆ°ä»»ä½•IPåœ°å€ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        sys.exit(1)
    print(f"âœ¨ æˆåŠŸæŠ“å–åˆ° {len(live_ips)} ä¸ªä¸é‡å¤çš„IPåœ°å€ã€‚")

    # 2. è¯»å–ä¼ªè£…åŸŸå
    host_domains = read_domains()
    if not host_domains:
        print("âŒ åŸŸåæ–‡ä»¶ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        sys.exit(1)
    print(f"ğŸ“‚ æˆåŠŸè¯»å–åˆ° {len(host_domains)} ä¸ªä¼ªè£…åŸŸåã€‚")
    
    # 3. ä¸ºæ¯ä¸ªIPç”ŸæˆVLESSé“¾æ¥
    all_vless_links = []
    print("\nğŸ”§ å¼€å§‹ä¸ºæ¯ä¸ªIPç”ŸæˆVLESSèŠ‚ç‚¹...")
    
    for ip in sorted(live_ips):
        address = ip
        random_host = random.choice(host_domains)
        
        # ä½¿ç”¨æ–°çš„å‰ç¼€æ¥æ ¼å¼åŒ–åˆ«å
        remarks = f"{REMARKS_PREFIX} IP[{address}] Host[{random_host}]"
        
        encoded_path = urllib.parse.quote(PATH)
        encoded_remarks = urllib.parse.quote(remarks)
        
        vless_link = (
            f"vless://{USER_ID}@{address}:{PORT}?"
            f"encryption=none&security=tls&type=ws&host={random_host}&path={encoded_path}"
            f"#{encoded_remarks}"
        )
        all_vless_links.append(vless_link)
        
    # 4. å°†æ‰€æœ‰ç”Ÿæˆçš„é“¾æ¥å†™å…¥æ–‡ä»¶
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write('\n'.join(all_vless_links))
        print(f"\nâœ… æˆåŠŸï¼æ‰€æœ‰ {len(all_vless_links)} æ¡èŠ‚ç‚¹é“¾æ¥å·²å†™å…¥åˆ° '{OUTPUT_FILE}' æ–‡ä»¶ä¸­ã€‚")
    except IOError as e:
        print(f"\nâŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)
